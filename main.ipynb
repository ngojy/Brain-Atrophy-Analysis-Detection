{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pydicom\n",
    "from pydicom import dcmread\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# device use CUDA\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# architecture\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting .dcm Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all .dcm files because this dataset is too big and I do not want to do all that sorting by hand\n",
    "\n",
    "# Define the parent directory where dataset are located\n",
    "parent_mri_dir = \"C:/Users/j/Downloads/MRI_PET_2D_Dataset/\"  # Change this to your actual dataset directory\n",
    "\n",
    "# Define source directories based on the parent directory\n",
    "source_mri_dirs = [os.path.join(parent_mri_dir, \"MRI_Norm\"), os.path.join(parent_mri_dir, \"MRI_AD\"),\n",
    "                   os.path.join(parent_mri_dir, \"PET_Norm\"), os.path.join(parent_mri_dir, \"PET_AD\")]\n",
    "\n",
    "# Define the destination directory\n",
    "destination_mri_root = os.path.join(parent_mri_dir, \"extracted\")\n",
    "\n",
    "# Ensure the destination root directory exists\n",
    "os.makedirs(destination_mri_root, exist_ok=True)\n",
    "\n",
    "for source in source_mri_dirs:\n",
    "    label = os.path.basename(source)  # Extracts \"Norm_MRI\" or \"AD_MRI\"\n",
    "    destination = os.path.join(destination_mri_root, label)\n",
    "    os.makedirs(destination, exist_ok=True)  # Create labeled destination folder\n",
    "\n",
    "    # Walk through all subdirectories\n",
    "    for root, _, files in os.walk(source):\n",
    "        for file in files:\n",
    "            if file.endswith(\".dcm\"):  # Check for .dcm files\n",
    "                src_path = os.path.join(root, file)\n",
    "                dst_path = os.path.join(destination, f\"{os.path.splitext(file)[0]}.png\")\n",
    "                \n",
    "                dicom_data = dcmread(src_path)\n",
    "                pixel_array = dicom_data.pixel_array\n",
    "\n",
    "                if len(pixel_array.shape) == 2:\n",
    "\n",
    "                    if os.path.exists(dst_path):\n",
    "                        print(f\"Skipping {file}, already extracted.\")\n",
    "                        continue\n",
    "\n",
    "                    plt.imshow(pixel_array, cmap=plt.cm.bone)\n",
    "    \n",
    "                    # Save the 2D image as a .png file in the new folder\n",
    "                    # output_path = os.path.join(dst_path, f\"{os.path.splitext(file)[0]}.png\")\n",
    "                    plt.imsave(dst_path, pixel_array, cmap='bone')  # Save using imsave\n",
    "                    plt.clf()  # Clear the plot to avoid overlapping images in the next iteration\n",
    "\n",
    "                    shutil.copy(src_path, dst_path)\n",
    "                    print(f\"Extracted: {file} to {dst_path} as .png\")\n",
    "\n",
    "print(\"Extraction complete. Files are sorted into labeled folders for images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 150547, Validation: 16728, Test: 41819\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"C:/Users/J/Downloads/MRI_PET_2D_Dataset\"\n",
    "categories = {\n",
    "    \"MRI_Norm\": 0, \"MRI_AD\": 1,\n",
    "    \"PET_Norm\": 0, \"PET_AD\": 1\n",
    "}\n",
    "\n",
    "data = []\n",
    "for category, label in categories.items():\n",
    "    files = glob.glob(os.path.join(base_dir, category, \"*.dcm\"))\n",
    "    data.extend([(f, label) for f in files])\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split into 80% train, 20% test\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split train into 90% test, 10% validation\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Validation: {len(val_data)}, Test: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainScanDataset:\n",
    "    def __init__(self, data_list, transform=None):\n",
    "        self.data_list = data_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.data_list[idx]\n",
    "        \n",
    "        # Load DICOM image\n",
    "        dicom_image = pydicom.dcmread(file_path).pixel_array\n",
    "\n",
    "        # Ensure native byte order (NumPy 2.0 compatible)\n",
    "        if dicom_image.dtype.byteorder not in ('=', '|'):\n",
    "            dicom_image = dicom_image.view(dicom_image.dtype.newbyteorder('='))\n",
    "\n",
    "        # Convert to float32 and normalize\n",
    "        dicom_image = dicom_image.astype(np.float32)\n",
    "        dicom_image = (dicom_image - dicom_image.min()) / (dicom_image.max() - dicom_image.min() + 1e-8)\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        image = torch.from_numpy(dicom_image).unsqueeze(0)  # Shape: (1, H, W)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((192, 192), antialias=True),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=1.5),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) \n",
    "])\n",
    "\n",
    "train_dataset = BrainScanDataset(train_data, transform=transform)\n",
    "val_dataset = BrainScanDataset(val_data, transform=transform)\n",
    "test_dataset = BrainScanDataset(test_data, transform=transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(\"  Data shape:\", data.shape)\n",
    "    print(\"  Targets shape:\", targets.shape)\n",
    "    print(\"  First data sample:\\n\", data[0])\n",
    "    print(\"  First target sample:\\n\", targets[0])\n",
    "\n",
    "    if batch_idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Neuroimages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the file is a directory, we need to use os.listdir() to get the .dcm files inside the directory\n",
    "# pydicom.dcmread is trying to read a file and not a directory which our file is.\n",
    "\n",
    "# for mri normal images\n",
    "dicom_MRI_Norm_dir = \"C:/Users/J/Downloads/MRI_PET_2D_Dataset/MRI_Norm\"\n",
    "dicom_MRI_Norm_files = [f for f in os.listdir(dicom_MRI_Norm_dir) if f.endswith(\".dcm\")]\n",
    "\n",
    "random_int = random.randint(0, len(dicom_MRI_Norm_files))\n",
    "\n",
    "if dicom_MRI_Norm_files:\n",
    "    dicom_file = os.path.join(dicom_MRI_Norm_dir , dicom_MRI_Norm_files[random_int])\n",
    "    dataset = pydicom.dcmread(dicom_file)\n",
    "\n",
    "    pixel_array = dataset.pixel_array\n",
    "\n",
    "    if len(pixel_array.shape) == 2:\n",
    "        plt.imshow(dataset.pixel_array, cmap=plt.cm.bone)\n",
    "\n",
    "    plt.title(\"Random MRI Normal DICOM Image\")\n",
    "    plt.axis(\"off\")\n",
    "    image_shape = dataset.pixel_array.shape\n",
    "    print(\"Original MRI Normal Image Shape:\", image_shape)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"No DICOM files found in the directory!\")\n",
    "\n",
    "\n",
    "\n",
    "# for mri ad images\n",
    "dicom_MRI_AD_dir = \"C:/Users/J/Downloads/MRI_PET_2D_Dataset/MRI_AD\"\n",
    "dicom_MRI_AD_files = [f for f in os.listdir(dicom_MRI_AD_dir) if f.endswith(\".dcm\")]\n",
    "\n",
    "random_int = random.randint(0, len(dicom_MRI_AD_files))\n",
    "\n",
    "if dicom_MRI_AD_files:\n",
    "    dicom_file = os.path.join(dicom_MRI_AD_dir , dicom_MRI_AD_files[random_int])\n",
    "    dataset = pydicom.dcmread(dicom_file)\n",
    "\n",
    "    pixel_array = dataset.pixel_array\n",
    "\n",
    "    if len(pixel_array.shape) == 2:\n",
    "        plt.imshow(dataset.pixel_array, cmap=plt.cm.bone)\n",
    "\n",
    "    plt.title(\"Random MRI AD DICOM Image\")\n",
    "    plt.axis(\"off\")\n",
    "    image_shape = dataset.pixel_array.shape\n",
    "    print(\"Original MRI AD Image Shape:\", image_shape)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No DICOM files found in the directory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model\n",
    "class CNN_model(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_model, self).__init__()\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "\n",
    "        self.maxpool_1 = torch.nn.MaxPool2d(2, 2)\n",
    "\n",
    "\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0) \n",
    "\n",
    "        self.maxpool_2 = torch.nn.MaxPool2d(2, 2)\n",
    "\n",
    "\n",
    "        self.conv_3 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=0) \n",
    "\n",
    "        self.maxpool_3 = torch.nn.MaxPool2d(2, 2)\n",
    "\n",
    "\n",
    "        self.conv_4 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=0) \n",
    "\n",
    "        self.maxpool_4 = torch.nn.MaxPool2d(2, 2)\n",
    "\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(128 * 8 * 8, 1024)\n",
    "\n",
    "        self.fc2 = torch.nn.Linear(1024, num_classes)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv_1(x))\n",
    "        # output: [(batch_size)128, 16, 188, 188]\n",
    "        out = self.maxpool_1(out)\n",
    "        # output: [94]\n",
    "\n",
    "        out = F.relu(self.conv_2(out))\n",
    "        # Output: [(batch_size)128, 32, 90, 90]\n",
    "        out = self.maxpool_2(out)\n",
    "        # Output: [45]\n",
    "\n",
    "        out = F.relu(self.conv_3(out))\n",
    "        # Output: [(batch_size)128, 64, 41, 41]\n",
    "        out = self.maxpool_3(out)\n",
    "        # Output: [20]\n",
    "\n",
    "        out = F.relu(self.conv_4(out))\n",
    "        # Output: [(batch_size)128, 128, 20, 20]\n",
    "        out = self.maxpool_4(out)\n",
    "        # Output: [8]\n",
    "\n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Training for [ 10 ] epochs\n",
      "[Epoch 1, Batch     1] loss: 0.007\n",
      "[Epoch 1, Batch   101] loss: 0.626\n",
      "[Epoch 1, Batch   201] loss: 0.590\n",
      "[Epoch 1, Batch   301] loss: 0.550\n",
      "[Epoch 1, Batch   401] loss: 0.541\n",
      "[Epoch 1, Batch   501] loss: 0.518\n",
      "[Epoch 1, Batch   601] loss: 0.503\n",
      "[Epoch 1, Batch   701] loss: 0.474\n",
      "[Epoch 1, Batch   801] loss: 0.453\n",
      "[Epoch 1, Batch   901] loss: 0.426\n",
      "[Epoch 1, Batch  1001] loss: 0.409\n",
      "[Epoch 1, Batch  1101] loss: 0.389\n",
      "[Epoch 2, Batch     1] loss: 0.004\n",
      "[Epoch 2, Batch   101] loss: 0.351\n",
      "[Epoch 2, Batch   201] loss: 0.327\n",
      "[Epoch 2, Batch   301] loss: 0.316\n",
      "[Epoch 2, Batch   401] loss: 0.296\n",
      "[Epoch 2, Batch   501] loss: 0.280\n",
      "[Epoch 2, Batch   601] loss: 0.267\n",
      "[Epoch 2, Batch   701] loss: 0.257\n",
      "[Epoch 2, Batch   801] loss: 0.240\n",
      "[Epoch 2, Batch   901] loss: 0.243\n",
      "[Epoch 2, Batch  1001] loss: 0.224\n",
      "[Epoch 2, Batch  1101] loss: 0.208\n",
      "[Epoch 3, Batch     1] loss: 0.001\n",
      "[Epoch 3, Batch   101] loss: 0.172\n",
      "[Epoch 3, Batch   201] loss: 0.170\n",
      "[Epoch 3, Batch   301] loss: 0.166\n",
      "[Epoch 3, Batch   401] loss: 0.154\n",
      "[Epoch 3, Batch   501] loss: 0.164\n",
      "[Epoch 3, Batch   601] loss: 0.150\n",
      "[Epoch 3, Batch   701] loss: 0.144\n",
      "[Epoch 3, Batch   801] loss: 0.145\n",
      "[Epoch 3, Batch   901] loss: 0.153\n",
      "[Epoch 3, Batch  1001] loss: 0.142\n",
      "[Epoch 3, Batch  1101] loss: 0.136\n",
      "[Epoch 4, Batch     1] loss: 0.001\n",
      "[Epoch 4, Batch   101] loss: 0.107\n",
      "[Epoch 4, Batch   201] loss: 0.115\n",
      "[Epoch 4, Batch   301] loss: 0.104\n",
      "[Epoch 4, Batch   401] loss: 0.109\n",
      "[Epoch 4, Batch   501] loss: 0.108\n",
      "[Epoch 4, Batch   601] loss: 0.111\n",
      "[Epoch 4, Batch   701] loss: 0.117\n",
      "[Epoch 4, Batch   801] loss: 0.105\n",
      "[Epoch 4, Batch   901] loss: 0.103\n",
      "[Epoch 4, Batch  1001] loss: 0.104\n",
      "[Epoch 4, Batch  1101] loss: 0.096\n",
      "[Epoch 5, Batch     1] loss: 0.000\n",
      "[Epoch 5, Batch   101] loss: 0.085\n",
      "[Epoch 5, Batch   201] loss: 0.089\n",
      "[Epoch 5, Batch   301] loss: 0.082\n",
      "[Epoch 5, Batch   401] loss: 0.077\n",
      "[Epoch 5, Batch   501] loss: 0.081\n",
      "[Epoch 5, Batch   601] loss: 0.078\n",
      "[Epoch 5, Batch   701] loss: 0.080\n",
      "[Epoch 5, Batch   801] loss: 0.080\n",
      "[Epoch 5, Batch   901] loss: 0.083\n",
      "[Epoch 5, Batch  1001] loss: 0.079\n",
      "[Epoch 5, Batch  1101] loss: 0.088\n",
      "[Epoch 6, Batch     1] loss: 0.001\n",
      "[Epoch 6, Batch   101] loss: 0.065\n",
      "[Epoch 6, Batch   201] loss: 0.065\n",
      "[Epoch 6, Batch   301] loss: 0.064\n",
      "[Epoch 6, Batch   401] loss: 0.081\n",
      "[Epoch 6, Batch   501] loss: 0.065\n",
      "[Epoch 6, Batch   601] loss: 0.063\n",
      "[Epoch 6, Batch   701] loss: 0.065\n",
      "[Epoch 6, Batch   801] loss: 0.062\n",
      "[Epoch 6, Batch   901] loss: 0.069\n",
      "[Epoch 6, Batch  1001] loss: 0.074\n",
      "[Epoch 6, Batch  1101] loss: 0.076\n",
      "[Epoch 7, Batch     1] loss: 0.001\n",
      "[Epoch 7, Batch   101] loss: 0.045\n",
      "[Epoch 7, Batch   201] loss: 0.041\n",
      "[Epoch 7, Batch   301] loss: 0.047\n",
      "[Epoch 7, Batch   401] loss: 0.046\n",
      "[Epoch 7, Batch   501] loss: 0.055\n",
      "[Epoch 7, Batch   601] loss: 0.065\n",
      "[Epoch 7, Batch   701] loss: 0.065\n",
      "[Epoch 7, Batch   801] loss: 0.055\n",
      "[Epoch 7, Batch   901] loss: 0.056\n",
      "[Epoch 7, Batch  1001] loss: 0.060\n",
      "[Epoch 7, Batch  1101] loss: 0.052\n",
      "[Epoch 8, Batch     1] loss: 0.001\n",
      "[Epoch 8, Batch   101] loss: 0.038\n",
      "[Epoch 8, Batch   201] loss: 0.038\n",
      "[Epoch 8, Batch   301] loss: 0.042\n",
      "[Epoch 8, Batch   401] loss: 0.044\n",
      "[Epoch 8, Batch   501] loss: 0.045\n",
      "[Epoch 8, Batch   601] loss: 0.048\n",
      "[Epoch 8, Batch   701] loss: 0.054\n",
      "[Epoch 8, Batch   801] loss: 0.055\n",
      "[Epoch 8, Batch   901] loss: 0.039\n",
      "[Epoch 8, Batch  1001] loss: 0.036\n",
      "[Epoch 8, Batch  1101] loss: 0.049\n",
      "[Epoch 9, Batch     1] loss: 0.000\n",
      "[Epoch 9, Batch   101] loss: 0.031\n",
      "[Epoch 9, Batch   201] loss: 0.029\n",
      "[Epoch 9, Batch   301] loss: 0.048\n",
      "[Epoch 9, Batch   401] loss: 0.042\n",
      "[Epoch 9, Batch   501] loss: 0.036\n",
      "[Epoch 9, Batch   601] loss: 0.043\n",
      "[Epoch 9, Batch   701] loss: 0.050\n",
      "[Epoch 9, Batch   801] loss: 0.043\n",
      "[Epoch 9, Batch   901] loss: 0.043\n",
      "[Epoch 9, Batch  1001] loss: 0.042\n",
      "[Epoch 9, Batch  1101] loss: 0.039\n",
      "[Epoch 10, Batch     1] loss: 0.001\n",
      "[Epoch 10, Batch   101] loss: 0.025\n",
      "[Epoch 10, Batch   201] loss: 0.025\n",
      "[Epoch 10, Batch   301] loss: 0.029\n",
      "[Epoch 10, Batch   401] loss: 0.038\n",
      "[Epoch 10, Batch   501] loss: 0.038\n",
      "[Epoch 10, Batch   601] loss: 0.038\n",
      "[Epoch 10, Batch   701] loss: 0.037\n",
      "[Epoch 10, Batch   801] loss: 0.031\n",
      "[Epoch 10, Batch   901] loss: 0.037\n",
      "[Epoch 10, Batch  1001] loss: 0.043\n",
      "[Epoch 10, Batch  1101] loss: 0.035\n",
      "CNN Training Finished\n"
     ]
    }
   ],
   "source": [
    "# training CNN model\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "model = CNN_model(num_classes=num_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(device)\n",
    "print(f'Training for [ {num_epochs} ] epochs')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        features, targets = features.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(features)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('[Epoch %d, Batch %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('CNN Training Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, targets in val_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Accuracy Calculation\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    return avg_val_loss, val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_model_v1_10_epochs.pth saved successfully.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), f\"cnn_model_v1_{num_epochs}_epochs.pth\")\n",
    "print(f\"cnn_model_v1_{num_epochs}_epochs.pth saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_model_v1_10_epochs.pth loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = CNN_model(num_classes=num_classes).to(device)\n",
    "loaded_model.load_state_dict(torch.load(f\"cnn_model_v1_{num_epochs}_epochs.pth\"))\n",
    "loaded_model.eval()\n",
    "\n",
    "print(f\"cnn_model_v1_{num_epochs}_epochs.pth loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy of model\n",
    "def compute_accuracy(model, data_loader):\n",
    "    correct=0\n",
    "    total=0\n",
    "\n",
    "    for batch_idx, (features, targets) in enumerate(data_loader):\n",
    "        features, targets = features.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "    acc = (correct/total) * 100\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sensitivity (recall)\n",
    "def compute_sensitivity(model, data_loader):\n",
    "    truePositive=0\n",
    "    falseNegative=0\n",
    "\n",
    "    for batch_idx, (features, targets) in enumerate(data_loader):\n",
    "        features, targets = features.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        truePositive += ((predicted == 1) & (targets == 1)).sum().item()\n",
    "        falseNegative += ((predicted == 0) & (targets == 1)).sum().item()\n",
    "\n",
    "    sens = truePositive / (truePositive + falseNegative) * 100\n",
    "\n",
    "    return sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute specificity \n",
    "def compute_specificity(model, data_loader):\n",
    "    trueNegative=0\n",
    "    falsePositive=0\n",
    "\n",
    "    for batch_idx, (features, targets) in enumerate(data_loader):\n",
    "        features, targets = features.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        trueNegative += ((predicted == 0) & (targets == 0)).sum().item()\n",
    "        falsePositive += ((predicted == 1) & (targets == 0)).sum().item()\n",
    "\n",
    "    spec = trueNegative / (trueNegative + falsePositive) * 100\n",
    "\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute f1-score\n",
    "def compute_f1_score(model, data_loader):\n",
    "    truePositive=0\n",
    "    trueNegative=0\n",
    "    falsePositive=0\n",
    "    falseNegative=0\n",
    "\n",
    "    for batch_idx, (features, targets) in enumerate(data_loader):\n",
    "        features, targets = features.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        truePositive += ((predicted == 1) & (targets == 1)).sum().item()\n",
    "        trueNegative += ((predicted == 0) & (targets == 0)).sum().item()\n",
    "\n",
    "        falsePositive += ((predicted == 1) & (targets == 0)).sum().item()\n",
    "        falseNegative += ((predicted == 0) & (targets == 1)).sum().item()\n",
    "\n",
    "        precision = truePositive / (truePositive + falseNegative)\n",
    "\n",
    "        sensitivity = truePositive / (truePositive + falseNegative)\n",
    "\n",
    "    f1 = (2 * precision * sensitivity) / (precision + sensitivity) * 100\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.42%\n",
      "Test sensitivity: 94.41%\n",
      "Test specificity: 97.71%\n",
      "Test f1-score: 94.41%\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))\n",
    "print('Test sensitivity: %.2f%%' % (compute_sensitivity(model, test_loader)))\n",
    "print('Test specificity: %.2f%%' % (compute_specificity(model, test_loader)))\n",
    "print('Test f1-score: %.2f%%' % (compute_f1_score(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "for batch_idx, (features, targets) in enumerate(test_loader):\n",
    "    features, targets = features.to(device), targets.to(device)\n",
    "\n",
    "    outputs = model(features)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "cm = confusion_matrix(targets, predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"//\"\n",
    "dcm_data = pydicom.dcmread(img_path)\n",
    "pixel_array = dcm_data.pixel_array\n",
    "\n",
    "image = Image.fromarray(pixel_array.astype(np.uint8)).convert(\"L\")\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((192, 192), antialias=True),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=1.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "plt.imshow(pixel_array, cmap=plt.cm.bone)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "loaded_model = CNN_model(num_classes=num_classes).to(device)\n",
    "loaded_model.load_state_dict(torch.load(\"cnn_model_v1.pth\"))\n",
    "loaded_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = loaded_model(image_tensor)\n",
    "    prediction = torch.argmax(output, dim=1)\n",
    "    print(f\"Predicted class: {prediction.item()}\")\n",
    "\n",
    "if prediction.item() == 0:\n",
    "    print(\"No brain atrophy detected.\")\n",
    "else:\n",
    "    print(\"Brain atrophy detected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
